{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, Normalize\n",
    "\n",
    "from data_generation.image_classification import generate_dataset\n",
    "from helpers import index_splitter, make_balanced_sampler\n",
    "from stepbystep.v1 import StepByStep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 6, 6)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single=np.array([[[[5,0,8,7,8,1],\n",
    "                   [1,9,5,0,7,7],\n",
    "                   [6,0,2,4,6,6],\n",
    "                   [9,7,6,6,8,4],\n",
    "                   [8,3,8,5,1,3],\n",
    "                   [7,2,7,0,1,0]]]])\n",
    "\n",
    "single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 3, 3)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity=np.array([[0,0,0],\n",
    "                  [0,1,0],\n",
    "                  [0,0,0]])\n",
    "identity=identity.reshape(1,1,3,3)\n",
    "identity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolving (Applying filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region=single[:,:,:3,:3] # NCHW shape\n",
    "\n",
    "filtered_region=region*identity\n",
    "total=filtered_region.sum()\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the movement in pixels called `stride`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_region=single[:,:,:3,1:4]\n",
    "new_filtered_region=new_region*identity\n",
    "new_total=new_filtered_region.sum()\n",
    "new_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger the filter, smaller the resulting image  \n",
    "\n",
    "$(h_i,w_i)*(h_f,w_f)=(h_i-(h_f-1),w_i-(w_f-1))$\n",
    "$\\\\(h_i,w_i)*f=(h_i-f+1,w_i-f+1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolving in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=torch.as_tensor(single).float()\n",
    "kernel_identity=torch.as_tensor(identity).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 5., 0., 7.],\n",
       "          [0., 2., 4., 6.],\n",
       "          [7., 6., 6., 8.],\n",
       "          [3., 8., 5., 1.]]]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolved=F.conv2d(image,kernel_identity,stride=1)\n",
    "convolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional module: Learn kernel/filter on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5369,  1.9690, -1.1619, -3.1844],\n",
       "          [-0.8648, -4.0165, -0.3939, -0.4451],\n",
       "          [-2.0863, -0.3478, -2.1432, -0.7654],\n",
       "          [-2.5016, -2.5363, -1.0745, -1.8194]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv=nn.Conv2d(in_channels=1,out_channels=1,kernel_size=3,stride=1)\n",
    "conv(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn multiple filters at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.3129, -0.0337, -0.0646],\n",
       "          [ 0.2135,  0.3195, -0.1148],\n",
       "          [-0.0616,  0.1517, -0.0329]]],\n",
       "\n",
       "\n",
       "        [[[-0.1778, -0.3059, -0.1706],\n",
       "          [ 0.0718,  0.2182,  0.3205],\n",
       "          [-0.1526, -0.0602, -0.1332]]]], requires_grad=True)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_multiple=nn.Conv2d(in_channels=1,out_channels=2,kernel_size=3,stride=1)\n",
    "conv_multiple.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use convolutional module to use particular weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 5., 0., 7.],\n",
       "          [0., 2., 4., 6.],\n",
       "          [7., 6., 6., 8.],\n",
       "          [3., 8., 5., 1.]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    conv.weight[0]=kernel_identity\n",
    "    conv.bias[0]=0\n",
    "\n",
    "conv(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(h_i,w_i)*f=(\\frac{h_i-f+1}{s},\\frac{w_i-f+1}{s})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 0.],\n",
       "          [7., 6.]]]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolution_stride2=F.conv2d(image,kernel_identity,stride=2)\n",
    "convolution_stride2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding; to preserve original size of the image after convolution\n",
    "Expand the input image: Add zero rows and columns around the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
       "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
       "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
       "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# symmetric padding\n",
    "constant_padder=nn.ConstantPad2d(padding=1,value=0.0) # padding: num of columns and rows to be stuffed, value: value that filling the new cols and rows\n",
    "constant_padder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 8])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_padder(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 0.],\n",
       "          [0., 1., 9., 5., 0., 7., 7., 0.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 0.],\n",
       "          [0., 9., 7., 6., 6., 8., 4., 0.],\n",
       "          [0., 8., 3., 8., 5., 1., 3., 0.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asymetric padding(change pad)\n",
    "asy_padded=F.pad(image,pad=(1,1,1,1),mode='constant',value=0) # pad=(left,right, top, bottom)\n",
    "asy_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other padding modes: replicate, reflect, circular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[5., 5., 0., 8., 7., 8., 1., 1.],\n",
       "          [5., 5., 0., 8., 7., 8., 1., 1.],\n",
       "          [1., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [9., 9., 7., 6., 6., 8., 4., 4.],\n",
       "          [8., 8., 3., 8., 5., 1., 3., 3.],\n",
       "          [7., 7., 2., 7., 0., 1., 0., 0.],\n",
       "          [7., 7., 2., 7., 0., 1., 0., 0.]]]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replication_padder=nn.ReplicationPad2d(padding=1)\n",
    "replication_padder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [0., 5., 0., 8., 7., 8., 1., 8.],\n",
       "          [9., 1., 9., 5., 0., 7., 7., 7.],\n",
       "          [0., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [7., 9., 7., 6., 6., 8., 4., 8.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 1.],\n",
       "          [2., 7., 2., 7., 0., 1., 0., 1.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 1.]]]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reflection_padder=nn.ReflectionPad2d(padding=1)\n",
    "reflection_padder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 7., 2., 7., 0., 1., 0., 7.],\n",
       "          [1., 5., 0., 8., 7., 8., 1., 5.],\n",
       "          [7., 1., 9., 5., 0., 7., 7., 1.],\n",
       "          [6., 6., 0., 2., 4., 6., 6., 6.],\n",
       "          [4., 9., 7., 6., 6., 8., 4., 9.],\n",
       "          [3., 8., 3., 8., 5., 1., 3., 8.],\n",
       "          [0., 7., 2., 7., 0., 1., 0., 7.],\n",
       "          [1., 5., 0., 8., 7., 8., 1., 5.]]]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circular_padding=nn.CircularPad2d(padding=1)\n",
    "circular_padding(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(h_i,w_i)*f=(\\frac{h_i+2p-f+1}{s},\\frac{w_i+2p-f+1}{s})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 3])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge=np.array([[[[0,1,0],\n",
    "                 [1,-4,1],\n",
    "                 [0,1,0]]]])\n",
    "\n",
    "kernel_edge=torch.as_tensor(edge).float()\n",
    "kernel_edge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-19.,  22., -20., -12., -17.,  11.],\n",
       "          [ 16., -30.,  -1.,  23.,  -7., -14.],\n",
       "          [-14.,  24.,   7.,  -2.,   1.,  -7.],\n",
       "          [-15., -10.,  -1.,  -1., -15.,   1.],\n",
       "          [-13.,  13., -11.,  -5.,  13.,  -7.],\n",
       "          [-18.,   9., -18.,  13.,  -3.,   4.]]]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_image=F.pad(image,pad=(1,1,1,1),mode='constant',value=0.0)\n",
    "conv_padded=F.conv2d(input=padded_image,weight=kernel_edge,stride=1)\n",
    "conv_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling: Shrinking images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[22., 23., 11.],\n",
       "          [24.,  7.,  1.],\n",
       "          [13., 13., 13.]]]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled=F.max_pool2d(input=conv_padded,kernel_size=2)\n",
    "pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[24.]]]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4x4 pooling\n",
    "maxpool4=nn.MaxPool2d(kernel_size=4)\n",
    "maxpool4(conv_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[24., 24., 23., 23.],\n",
       "          [24., 24., 23., 23.],\n",
       "          [24., 24., 13., 13.],\n",
       "          [13., 13., 13., 13.]]]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.max_pool2d(input=conv_padded,kernel_size=3,stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22., 23., 11., 24.,  7.,  1., 13., 13., 13.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened=nn.Flatten(1,-1)(pooled)\n",
    "flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22., 23., 11., 24.,  7.,  1., 13., 13., 13.]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled.view(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Architecture  \n",
    "\n",
    "**Typical Convolutional block**: Preprocessing images and coverting them into features\n",
    "1. Convolution\n",
    "2. Activation function\n",
    "3. Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5\n",
    "lenet=nn.Sequential()\n",
    "\n",
    "# Featurizer\n",
    "# block 1: 1@28x28-->6@28x28-->6@14x14\n",
    "lenet.add_module('conv2d1',nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5,padding=2))\n",
    "lenet.add_module('activation1',nn.ReLU())\n",
    "lenet.add_module('maxpool2d1',nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "# block 2: 6@14x14-->16@10x10-->16@5x5\n",
    "lenet.add_module('conv2d2',nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5))\n",
    "lenet.add_module('activation2',nn.ReLU())\n",
    "lenet.add_module('maxpool2d2',nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "# block 3: 16@5x5-->120@1x1\n",
    "lenet.add_module('conv2d3',nn.Conv2d(in_channels=16,out_channels=120,kernel_size=5))\n",
    "lenet.add_module('activation3',nn.ReLU())\n",
    "lenet.add_module('flatten',nn.Flatten())\n",
    "\n",
    "# Classification\n",
    "# Hidden layer\n",
    "lenet.add_module('linear1',nn.Linear(in_features=120,out_features=84))\n",
    "# output layer\n",
    "lenet.add_module('linear2',nn.Linear(in_features=84,out_features=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv2d1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (activation1): ReLU()\n",
       "  (maxpool2d1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2d2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (activation2): ReLU()\n",
       "  (maxpool2d2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2d3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (activation3): ReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear1): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (linear2): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel(Horizontal or vertical) ---> 0  \n",
    "Diagonal(Tilted to the right) ---> 1  \n",
    "Diagonal(Tileted to the left) ---> 2  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "images,labels=generate_dataset(img_size=10,n_images=1000,binary=False,seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig=plot_images(images,labels,n_plot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedTensorDataset(Dataset):\n",
    "    def __init__(self,x,y,transform=None):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.transform=transform\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        x=self.x[index]\n",
    "        if self.transform:\n",
    "            x=self.transform(x)\n",
    "        return x,self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tensors from numpy arrays before split\n",
    "# Modify pixel values from [0,255] to [0,1]\n",
    "X_tensor=torch.as_tensor((images/255),dtype=torch.float32)\n",
    "y_tensor=torch.as_tensor(labels,dtype=torch.long)\n",
    "\n",
    "# Uses index splitter to generate indices for training and validation sets\n",
    "train_idx,val_idx=index_splitter(len(X_tensor),[80,20])\n",
    "# Uses indices to perform split\n",
    "X_train_tensor,y_train_tensor=X_tensor[train_idx],y_tensor[train_idx]\n",
    "X_val_tensor,y_val_tensor=X_tensor[val_idx],y_tensor[val_idx]\n",
    "\n",
    "# we are not doing any data augmentation\n",
    "train_composer=Compose([Normalize(mean=(0.5),std=(0.5))])\n",
    "val_composer=Compose([Normalize(mean=(0.5),std=(0.5))])\n",
    "\n",
    "# Uses custom dataset to apply composed transforms\n",
    "train_dataset=TransformedTensorDataset(x=X_train_tensor,y=y_train_tensor,transform=train_composer)\n",
    "val_dataset=TransformedTensorDataset(x=X_val_tensor,y=y_val_tensor,transform=val_composer)\n",
    "\n",
    "# Build weighted random sampler to handle imbalanced classes\n",
    "sampler=make_balanced_sampler(y_train_tensor)\n",
    "\n",
    "# Uses sampler on the training set to get a balanced DataLoader\n",
    "train_loader=DataLoader(dataset=train_dataset,sampler=sampler,batch_size=16)\n",
    "val_loader=DataLoader(dataset=val_dataset,batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf {Softmax} \\\\$\n",
    "$z = logit(p) =\\text{log odds ratio(p)}=log\\frac{p}{1-p}\\\\$\n",
    "$e^z = e^{\\text{logit}(p)} = \\text{odds ratio}(p)=\\frac{p}{1-p} \\\\$\n",
    "$Softmax(z_i)=\\frac{e^z_i}{\\sum_{c=0}^{N_c-1} e^{z_c}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.0000, 1.0000, 0.5000])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits=torch.tensor([1.3863,0.0000,-0.6931])\n",
    "odd_ratios=torch.exp(logits)\n",
    "odd_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7273, 0.1818, 0.0909])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmaxed=odd_ratios/odd_ratios.sum()\n",
    "softmaxed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7273, 0.1818, 0.0909])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(dim=-1)(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7273, 0.1818, 0.0909])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits,dim=-1) # siftmax applies to last dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3185, -1.7048, -2.3979])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs=F.log_softmax(logits,dim=-1) # log probs of classes for a single data point\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3979)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets assume its label is 2\n",
    "label=torch.tensor([2])\n",
    "F.nll_loss(log_probs.view(-1,3),label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5229, -0.3146, -2.9600],\n",
       "        [-1.7934, -1.0044, -0.7607],\n",
       "        [-1.2513, -1.0136, -1.0471],\n",
       "        [-2.6799, -0.2219, -2.0367],\n",
       "        [-1.0728, -1.9098, -0.6737]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(11)\n",
    "dummy_logits=torch.randn((5,3))\n",
    "dummy_labels=torch.tensor([0,0,1,2,1])\n",
    "dummy_log_probs=F.log_softmax(dummy_logits,dim=-1)\n",
    "dummy_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6553)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_log_probs=torch.tensor([-1.5229,-1.7934,-1.0136,-2.0367,-1.9098])\n",
    "-relevant_log_probs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6553)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn=nn.NLLLoss()\n",
    "loss_fn(dummy_log_probs,dummy_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7188)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn=nn.NLLLoss(weight=torch.as_tensor([1,1,2],dtype=torch.float32)) # since we need to balance out dataset\n",
    "loss_fn(dummy_log_probs,dummy_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5599)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignore datapoints with label(y=2)\n",
    "loss_fn=nn.NLLLoss(ignore_index=2)\n",
    "loss_fn(dummy_log_probs,dummy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logits-->`nn.LogSoftmax()`-->log probabilities-->`nn.NLLLoss()`  |  (If last layer is nn.LogSoftmax())    \n",
    "logits-->nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6553)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(11)\n",
    "dummy_logits=torch.randn((5,3))\n",
    "dummy_labels=torch.tensor([0,0,1,2,1])\n",
    "\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "loss_fn(dummy_logits,dummy_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "# featurizer\n",
    "# Block1: 1@10x10 -> n_channels@8x8 -> n_channels@4x4\n",
    "n_channels=1\n",
    "model_cnn1=nn.Sequential()\n",
    "model_cnn1.add_module('conv1',nn.Conv2d(in_channels=1,out_channels=n_channels,kernel_size=3))\n",
    "model_cnn1.add_module('relu1',nn.ReLU())\n",
    "model_cnn1.add_module('maxp1',nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "# Flattening: n_channels _ 4 _ 4\n",
    "model_cnn1.add_module('flatten',nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (maxp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "model_cnn1.add_module('fc1',nn.Linear(in_features=n_channels*4*4,out_features=10))\n",
    "model_cnn1.add_module('relu2',nn.ReLU())\n",
    "model_cnn1.add_module('fc2',nn.Linear(in_features=10,out_features=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "lr=0.1\n",
    "multi_loss_fn=nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer=optim.SGD(model_cnn1.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_cnn1=StepByStep(model=model_cnn1,loss_fn=multi_loss_fn,optimizer=optimizer)\n",
    "sbs_cnn1.set_loaders(train_loader,val_loader)\n",
    "sbs_cnn1.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
